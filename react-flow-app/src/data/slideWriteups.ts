export const slideWriteups: Record<string, string> = {
  "slide-01": `Every major technology has widened the gap between people who adopt it and people who don't. Fire, bicycles, cars, the internet -- each one gave its users capabilities and speed that everyone else simply didn't have. If you have a bike, you can get places people on foot can't. If you have the internet, you can access information that someone without it never will. That's always been the pattern.

What makes AI different is the sheer velocity of change. Previous technology gaps grew slowly -- the internet took decades to become indispensable. AI is compressing that timeline dramatically. The people using AI tools right now aren't just a little faster or a little more capable; they're operating in a fundamentally different mode. They can research, build, create, and automate in ways that simply weren't possible a year ago.

The uncomfortable truth is that this isn't a gap that will stabilize on its own. It's widening. And unlike previous tech shifts where you could afford to wait and see, the speed here means every month you delay is a month where the gulf grows. The good news, as we'll explore in [what it takes](/resources#slide-06) and [mapping the journey](/resources#slide-05), is that the barrier to crossing this divide is lower than you might think. You don't need to be "technical." You just need to be willing to start.`,

  "slide-06": `The word "technical" used to mean something very specific. It meant you could write code -- you were a front-end engineer, a back-end developer, someone who understood programming languages. That was the barrier to entry for building things with computers, and it kept a lot of people out.

That barrier has fundamentally shifted. What it means to be "technical" is now amorphous, and it's mostly about willingness. Are you willing to sit down and try something new? Are you willing to fumble through unfamiliar tools? Because the tools themselves have gotten so much more accessible that the old gatekeeping around "technical ability" just doesn't hold anymore. You have an ever-present thing that can break concepts down, build things for you, teach you as you go.

The real barrier now is resistance to change. And I get it -- change is uncomfortable, especially when it's moving this fast. But resistance to learning AI tools isn't really about capability. It's about [avoidance](/resources#slide-07), and avoidance is just fear wearing a practical disguise. The people who will thrive aren't the ones who were already "technical." They're the ones who decided to engage with something new and gave themselves permission to be bad at it for a while.`,

  "slide-02": `Think about how you learned to use Google. Nobody handed you an instruction manual. Nobody sat you down for a course on search operators and boolean logic. You just used it, over and over, and eventually you built an intuition for what kinds of queries would give you good results and which ones wouldn't.

That intuition is your mental model -- an internal sense of what a tool can and can't do, how it behaves, and where its edges are. Here's the thing: mental models improve far more through use than through instruction. I like to use this example with my family. If I ask my parents to Google directions to an ice cream shop, they can absolutely do that. But if I asked them to find our sister's Facebook photos using Google, they might try it -- because to someone without a refined mental model of Google, that seems like a perfectly reasonable request. Photos exist on the internet. Google searches the internet. Why wouldn't that work?

For those of us who grew up with Google, the answer is obvious. But it's only obvious because we used it enough to understand its boundaries. The exact same principle applies to AI. You don't need to read articles about how large language models work. You need to use them -- a lot. The more you use them, the more your mental model sharpens, and the better you get at knowing [when to treat them like Google and when not to](/resources#slide-03).`,

  "slide-03": `There's a billboard in San Francisco promoting ChatGPT for fixing your broken car. It's positioning AI as this all-knowing oracle you can consult for answers -- basically a better version of Google. And honestly, that's the most natural mental model to start with. You type a question, you get an answer. Sounds like search, but smarter.

The problem is that Google and AI have completely different failure modes. Google's limitations are about access -- it can't search your Gmail, it can't see behind paywalls, it can't access private databases. You learn those boundaries pretty quickly. AI's big flaw is hallucination: it will confidently tell you something that is completely made up, and it will look indistinguishable from a correct answer. That's a fundamentally different kind of wrong than "no results found."

If you're using ChatGPT with a Google mental model, you're going to trust answers you shouldn't and miss capabilities you don't know about. The real power of these tools emerges when you update your mental model to match what they actually are -- which is less like a search engine and more like a [digital employee](/resources#slide-04) that can reason, create, and execute. Building an accurate mental map of what these tools can do is one of the most valuable investments you can make, and the only way to build it is through consistent use.`,

  "slide-04": `Most people are stuck at the "savant" stage. They use AI the way it's been marketed to them: as this incredibly knowledgeable entity that you ask questions and get text answers back. Need advice on a topic? Ask the AI. Want something explained? Ask the AI. That's valuable, but it's a fraction of what's actually possible.

The major mental leap is realizing that AI is not just a savant dispensing wisdom -- it's a digital employee that can execute actual work. It can write code, build documents, analyze data, create images, draft plans, and iterate on all of the above. The shift from "I ask, it answers" to "I direct, it produces" is the single biggest unlock for most people. A lot of people get stuck at the chat stage and never make this jump.

Once you make that leap, your relationship with AI changes entirely. You stop being someone who consults an oracle and start being something more like a team lead. You're giving direction, reviewing output, asking for revisions, and orchestrating work. You don't need to know how to do everything yourself -- you need to know what you want done and be able to evaluate whether it was done well. That's a skill set most people already have from their professional lives. The key is recognizing that it applies here too, and then [mapping out the steps](/resources#slide-05) to get there.`,

  "slide-05": `If you've tried to learn about AI and felt overwhelmed, I want you to know: that's not a you problem. There is an absurd amount of information out there -- new tools, new models, new techniques, new frameworks -- and it's all moving at a speed that makes it impossible to absorb everything at once. The feeling of being confused or lost is incredibly common, and it's almost always a sequencing problem, not a capability problem.

Here's what I mean. If you skip too far ahead -- if you jump straight to AI coding agents without understanding how context works, or try to build automations before you're comfortable with basic prompting -- nothing feels coherent. Each concept depends on the ones before it. When I tried to skip ahead in my own learning, that's exactly where I got confused. It wasn't that the material was too hard. It was that I was missing foundational pieces.

That's the whole idea behind this presentation. Wherever you are on your journey, there's a logical next step. You don't need to understand everything -- you just need to know what comes next for you, and you need the [greatest self-learning tool ever created](/resources#slide-08) to help you get there. These AI models will patiently explain anything, answer questions you'd be embarrassed to ask a human, and walk you through concepts step by step. The path is there. You just need to follow it in order and resist the urge to [avoid it entirely](/resources#slide-07).`,

  "slide-07": `"This won't affect my job." I hear some version of that from a lot of people, and I understand the impulse. AI is moving fast, the discourse around it is noisy and sometimes alarming, and it's very easy to look at where you are professionally and think: I'm fine. I don't need to worry about this yet.

That reaction is avoidance, and avoidance is just fear showing its head in a socially acceptable way. Whether you're technical or not, the instinct to say "this doesn't apply to me" is natural when something feels confusing or threatening. But avoidance doesn't make the technology stop advancing. It just means the [widening gulf](/resources#slide-01) keeps growing while you stand still.

There's that line that gets repeated a lot: "AI won't take your job, but someone who uses AI will." I know it's played out, but I think it's genuinely true. AI may require us to think at a higher level -- to focus more on direction and judgment and less on manual execution. That's a shift, and it can feel uncomfortable. But the alternative is being the person who doesn't use the internet in 2005. The barrier isn't technical skill -- as we talked about in [what it takes](/resources#slide-06), it's really just willingness to engage. Step one is simply deciding not to look away.`,

  "slide-08": `This is where most people start, and honestly, it's a great place to be. You open up ChatGPT and ask it questions -- get summaries of articles, brainstorm ideas for a project, ask it to explain something you read. This is the portal to internet research, and it's where the majority of daily ChatGPT users still spend their time.

What makes this level interesting is that it's where prompt engineering starts to matter. Not the intimidating, academic version of prompt engineering -- just learning that the way you phrase a question changes the quality of the answer you get back. Instead of asking "what is inflation," you might say "explain inflation to me like I'm in high school, and give me three real-world examples." That small shift in how you ask is the beginning of developing a mental model for how these tools actually work.

The trap at this level is treating AI like a slightly better Google search. It's not -- it's a fundamentally different tool with different strengths and different failure modes. Google retrieves information that already exists on the web. AI generates responses based on patterns in its training. That distinction matters, especially when it comes to [understanding hallucinations](/resources#slide-03). But as a starting point for building the habit of reaching for AI when you have a question? This is where it all begins.`,

  "slide-09": `Once you move past basic Q&A, something shifts -- you start having actual conversations with the model. Instead of asking a single question and getting a single answer, you're going back and forth. You're dropping in PDFs or documents or images and saying "help me understand this." You might spend twenty minutes on a complex topic like a health question, where you're feeding it test results or research papers and having it walk you through what everything means.

This is where giving the model more context starts to pay off. The more information you provide -- whether it's a document you upload, a screenshot you paste in, or just a longer explanation of your situation -- the more useful the response becomes. You're treating the AI less like a search engine and more like a thought partner who happens to have read a lot of books.

It's also worth trying different models at this stage. ChatGPT, Gemini, and Claude each have their own personality and strengths. ChatGPT is great for web search and broad research. Gemini integrates tightly with Google's ecosystem. Claude tends to be more careful and thorough with long documents. Trying all three is like working with different colleagues -- they each bring something different to the table. Features like deep research, thinking mode, and web search within these tools start to become genuinely useful once you're having richer conversations. The desktop apps for both ChatGPT and Claude are worth installing too, since they make it easier to have the AI available alongside whatever you're working on.`,

  "slide-11": `At this point you're using AI regularly, but you might still be getting inconsistent results. That's normal -- and that's why dedicated AI tools start to matter. These are purpose-built applications that solve specific friction points in how you interact with AI.

Wispr Flow is one I recommend constantly. It's a voice dictation tool, and it completely changes the dynamic. The slowest part of working with AI is typing out what you're thinking. With Wispr Flow, you just talk. You don't have to organize your thoughts first -- you just brain dump, and it transcribes everything. I find it dramatically faster because I don't have to decide what I'm thinking before I start. I just think out loud.

Granola handles automatic meeting notes, which is another huge quality-of-life improvement. And then there's Obsidian -- I moved all my notes from Apple Notes into Obsidian because it stores everything as plain Markdown files on your computer. That means AI tools can actually read them. I can point Claude Code at my Obsidian directory and ask it to do anything -- take all the recipes I've ever saved and categorize them, look through my holiday letters for patterns, analyze notes from negotiation clients and find commonalities. My entire digital life becomes accessible to AI in a way it simply can't be when everything is locked inside Apple Notes or Google Docs.

The bigger point here is that these tools start to reshape your daily workflow. Figma, Canva, Notion, Slack -- they're all adding AI features. But the dedicated tools like Wispr Flow and Obsidian are the ones that make the biggest difference because they remove friction at the exact moments where you're trying to get information into or out of the model. And I'm still barely scratching the surface of what's possible at this level.`,

  "slide-10": `This is the level where everything starts to click, and it's also the level where a bit of technical understanding goes a long way. At any given time, what you're doing with AI is taking a bunch of written information, breaking it into pieces called tokens, and feeding it through the model. The model has a limited memory -- a context window -- and when that memory gets full, your results degrade. A very long chat will start giving you poor answers not because the model got dumber, but because it's trying to hold too much in its head at once.

Understanding this changes everything. Before you get here, your mental model of the AI is off, and so your results feel unreliable. You might think "this model isn't very good" or "I tried it but it didn't really work." But the issue often isn't the model -- it's that you don't yet understand the constraints you're working within. Once your mental model matches reality, you start getting consistently useful output.

Structured input is a big part of this. Most people don't know what Markdown is, but learning it is one of the highest-leverage things you can do. If you give the model a PDF, that's actually way worse than giving it a Markdown file -- PDFs are essentially images, and language models are built to process text. A structured document in Markdown or XML is both human-readable and machine-readable, which means the model can work with it much more effectively. Learning to use Projects to manage context -- keeping related information together and starting fresh when a conversation gets bloated -- is what makes the difference between "AI is unreliable" and "AI is my most productive tool." This is where [AI as a thought partner](/resources#slide-09) starts to deliver on its promise, because now you know how to set up the conversation for success.`,

  "slide-12": `I think of the browser as the modern workbench. It's where most of us spend our working hours, and AI is being woven directly into that experience in ways that are genuinely transformative.

The most accessible example is Google Gemini built right into Chrome. You can enable it in your Chrome settings under AI, and once it's on, it's a huge unlock. When you're on any website and you don't understand something, Gemini can pull in the entire page -- you'll see a blue line indicating everything that's now in context. So you can just ask "explain this to me like I'm in high school" and it knows exactly what you're looking at. You never have to copy and paste anything. You never have to leave the page. Anything you see on a website, you never have to be confused by it again. Just that capability by itself is enormous.

Then there's ChatGPT's Operator (previously called Atlas), which isn't a Chrome extension -- it's actually a separate browser, like a fork of Chrome. It can navigate websites, fill out forms, and take actions on your behalf. And the Claude extension takes a similar approach, letting you interact with web pages directly through Claude's intelligence. The key insight across all of these is that the AI meets you where you already are -- in the browser -- rather than forcing you to context-switch into a separate chat window. When you combine this with [solid context management skills](/resources#slide-10), you start to see how AI can be woven into nearly every moment of your digital work.`,

  "slide-13": `Most people treat AI image generation as a curiosity. You type in "a cat wearing a top hat" and you get something kind of fun and you move on. That's fine, but the real skill here is something deeper: can you get it to give you what you actually want? That is a couple of hops further than just saying "I've tried it and it's kind of interesting." You need to learn about camera angles, lighting, composition, art styles -- or you don't learn about any of that yourself. You just ask the AI to teach you what you need to know, and then have it write the prompt for you. It's the same [meta-level thinking](/resources#slide-14b) pattern that shows up everywhere on this map.

The tools themselves are varied. Midjourney requires you to use Discord, which is its own learning curve. ChatGPT has image generation built right in. Nano Banana (Google's image generation through Gemini) is getting incredibly good. And the quality ceiling has risen dramatically -- it's very possible to generate stuff that is indistinguishable from what a human would create. It's also very possible to get a bunch of generic AI slop. The difference is entirely in the person directing it.

For this presentation, I actually built a whole image selection website rather than evaluating images one-by-one in the model. I had multiple models generate variations simultaneously, then reviewed them in a gallery format to pick the best ones. That's the kind of workflow that emerges when you start treating these tools seriously -- you build your own process around them instead of just accepting whatever comes out on the first try.`,

  "slide-14": `This is where you stop just getting information or media out of AI and start having it do actual tasks for you. Zapier is the most well-known automation tool, but honestly it's already been surpassed by what's available now. N8n and Gumloop are visual workflow builders -- you drag and drop blocks that represent actions, connect them together, and suddenly you have a pipeline that pulls data from one place, transforms it, and pushes it somewhere else. All without writing a line of code.

The key insight is that these are visual interfaces to doing things with code, without you ever looking at code. For a non-technical person, this is a huge unlock. You can build automations that scrape information, clean it up, send it to the right places, and run on a schedule -- things that previously required hiring a developer. As an example, I used automation tools to scrape my old blog posts from the Wayback Machine, clean them up, and rebuild my personal website. That's a real project with real results, not just a demo.

That said, I'll be honest: even these tools are starting to feel a bit antiquated. Once you get comfortable with [more advanced tools](/resources#slide-16), you can just describe the inputs and outputs you want and have AI handle the plumbing directly. But if you want to see what's happening and understand the flow visually, automation tools like N8n and Gumloop are a fantastic stepping stone. They build intuition for how data moves through systems, which matters at every level above this one.`,

  "slide-15": `This is almost the last stop on the completely non-technical track, and it's a big one. Tools like Lovable, Bolt AI, and Google AI Studio let you create actual software just by describing what you want in plain English. No syntax to memorize, no development environment to configure. You type something like "build me a landing page for my event with an RSVP form" and it starts generating a working website right in front of you.

Google AI Studio is a great example. It has a preview mode and a code mode -- you can watch it create a website in real time from a chat interface. I gave it almost nothing for my wedding site and it one-shot a solid starting point. In my opinion, there's no reason to ever use Squarespace or WordPress at this point. That world is effectively over. Why pay a monthly subscription for a template when you can describe exactly what you want and get something custom in minutes?

But here's the honest part: understanding the limits is genuinely part of the learning. These tools are great for front-end, visual stuff -- landing pages, simple interactive sites, demos and prototypes. They struggle with anything that needs a back-end, databases, or complex logic. My wedding site started in Google AI Studio and I finished it with [Claude Code](/resources#slide-16) when I needed more interactivity. That progression is natural and healthy. These natural language tools are a fantastic playground to get comfortable with the idea that you can make software. Once you feel that, the question isn't whether you can build things -- it's how far you want to take it.`,

  "slide-15b": `Claude Co-work is the newest installment of a pattern we keep seeing: take what developers are doing with AI and make it accessible through a graphical interface. With regular ChatGPT or Claude conversations, the AI doesn't have access to your computer. It can help you think and write, but it can't actually change files on your machine. Co-work changes that. You give it access to a folder on your computer, and it can read, organize, and modify the files in that folder.

I was using Co-work on this very presentation. I gave it access to the talk slides folder where all my development files live, and it could make decisions about the files -- organizing content, cleaning things up, restructuring data. It does better when the task isn't heavy coding work (there's not much point in using it for that when [CLI tools](/resources#slide-16) exist). Where it shines is with file organization, content editing, and tasks where you want an AI that can see and touch your actual work. Think of it as having a capable assistant sitting at your computer alongside you, rather than one you're talking to over the phone. That direct access to your files is what makes the difference.`,

  "slide-16": `This is where most people's eyes glaze over, and I get it. You see a black screen with text and your brain screams "this is not for me." But here is the thing I keep coming back to: you are still just typing in plain English. The command line interface is not a programming language. It is a text-based way to talk to your computer, and now that AI lives there, it is also a text-based way to talk to AI that has access to your entire machine. All the most powerful AI tools right now -- Claude Code, Gemini CLI, OpenAI Codex -- they all run in the terminal. If you want to move past chatting and start actually building, this is the door you walk through.

The basics are genuinely simple. There are three commands you need to start: \`pwd\` tells you where you are, \`ls\` shows you what is in that folder, and \`cd\` moves you to a different folder. That is it. It is the exact same thing as clicking around in Finder, just without the pictures. Once you are in a folder, you can run Claude Code and suddenly you have a ChatGPT-style conversation -- except this one can read your files, write code, and execute things on your machine.

I upgraded my terminal to Ghostty, which was a game changer in the same way that switching from Safari to Chrome was back in the day. Tabs, split screens, customization -- it makes the experience of living in the terminal feel modern instead of intimidating. The command line is not about memorizing arcane syntax. It is about having a text window where you and AI can get real work done together, with no guardrails and no limitations on what you can access.`,

  "slide-18": `Git and GitHub were a bigger unlock than I expected, and they took longer to really understand than I want to admit. I had used GitHub before, I thought I knew what it was. But actually understanding how versioning works -- where the head is, how branches split off, what merging means, how pull requests flow -- that was not intuitive for me at all. It is worth spending real time on this, because once it clicks, it changes everything about how you work with AI.

Think of Git like Google Docs version history, but for code. Every time you save a snapshot, you can always go back. You can branch off and try something experimental without risking your working version. And when AI makes a mistake -- which it will -- you just rewind to before the mistake happened. That safety net is essential. Without it, you are one bad AI suggestion away from losing hours of work. With it, you can let the AI be aggressive and creative knowing you can always undo.

GitHub is where it goes from useful to transformative. GitHub is basically Dropbox for code -- you store projects there, share them, and collaborate with others. But the real magic is access to what everyone else has built. When someone releases an open source project, the entire thing lives on GitHub. You can pull it down, point Claude Code at it, and have it explain every piece of what is happening. There is a README file that describes the project, and AI can walk you through the rest. It is the difference between writing books on your own and having access to every book in the library. That access, combined with [the command line](/resources#slide-16), is where things really open up.`,

  "slide-19": `An IDE -- Integrated Development Environment -- is where code actually gets written, and AI has completely changed what that experience looks like. If you have never coded before, the easiest place to start right now is probably Google's Anti-Gravity. It has something called an Agent Manager, and what makes it interesting is that there is no code on screen at all. You are just talking to it in natural language, the same way you would talk to ChatGPT, and it builds software for you without ever showing you a terminal. For someone who does not want to look at code, that is a genuinely comfortable entry point.

Cursor is another popular option and works well as a next step. It puts a chat window right next to a code editor, so you can see what the AI is writing in real time. You talk, it writes code, you see the result, it adjusts. That inline feedback loop is powerful -- the agent writes something, sees whether it worked, and then improves on it automatically. It is less like giving instructions and more like pair programming with someone who types incredibly fast.

That said, I think the trajectory for most people will eventually lead past Cursor. The challenge with Cursor is that it does not have its own AI models -- it is borrowing from Anthropic, OpenAI, and others. The value it adds is the interface and the integration, but as [the command line tools](/resources#slide-16) get better and the models themselves get more capable, the question of what a middleman IDE adds gets harder to answer. Start wherever you are comfortable, but know that the tools are evolving fast and the best place to be is wherever gives you the tightest loop between asking and seeing results.`,

  "slide-20": `Once you have an AI coding tool set up, the next level is realizing you are not just having a conversation -- you are running an actual software development lifecycle. Plan, build, test, review, deploy, check for bugs, refactor, and plan again. That full cycle is what professional developers do, and with AI, you can execute every step of it without being an expert in any single one. The key insight is that one model can shape-shift depending on what you need it to do at each stage.

The way this works in practice is through modes. Planning mode, for instance, will ask you a bunch of questions before writing a single line of code. It is trying to understand what you want, surface edge cases, and create a roadmap. Development mode is heads-down building. Code review mode reads what was built and looks for problems. Each mode gives the AI different context and different constraints, which means you get different quality outputs for different purposes. It is the same model wearing different hats.

This is a fundamentally different thing from freeform conversation. When you are just chatting back and forth, you are improvising. When you are working through plan, build, test, review, deploy -- you are executing a structured workflow. And the structured approach produces dramatically better results, especially for anything beyond a quick script. At every layer of this process, the question I keep asking myself is: how can I just be making decisions and not getting stuck learning the mechanics of each step? The AI handles the execution. You handle the judgment calls. That division of labor is what makes the whole [agents and coding](/resources#slide-19) paradigm actually work.`,

  "slide-21": `At some point your project has to leave your laptop and exist in the real world. Going live involves a whole stack of services, and the good news is that most of them have excellent CLI tools -- which means your AI coding agent can handle a lot of the setup for you. I use Vercel for hosting and deployment (it's serverless, so you don't manage servers yourself), Supabase for databases and authentication, and Cloudflare for domains and DNS. Cloudflare in particular has great security features and a generous free tier.

The real power move here is that instead of clicking through web dashboards, you can tell Claude Code to use the Vercel CLI, the Supabase CLI, or Cloudflare's Wrangler to configure everything from the terminal. The agent can set up your database tables, deploy your app, and connect your custom domain -- all in one session. It feels like magic the first time you watch it happen.

But here's the thing I want to stress: don't just press buttons. When the AI is running these deployment commands, pay attention to what it's doing. Read the output. Ask it to explain steps you don't understand. Deployment is one of those areas where things go wrong in subtle ways -- an environment variable not set, a DNS record pointing to the wrong place -- and if you've been passively clicking "approve" without understanding the process, you'll be lost when something breaks. The learning happens in the doing, and [going live](/resources#slide-21) is where the doing gets very real.`,

  "slide-22": `This is the slide where the difference between a 1x engineer and a 10x engineer lives. Customizing your AI harness -- the tools, configurations, and shortcuts that wrap around the model -- is where you go from using AI to truly wielding it. And the wild part is, you're often just one markdown file away from dramatically changing how the agent behaves.

Skills are probably the most exciting piece right now. A skill is basically a folder with markdown files that gets loaded at the moment it's needed -- like Neo downloading kung fu in The Matrix. You don't permanently carry every skill in context; you pull them in on demand. I have skills for generating image prompts, writing blog posts, running code reviews with competing sub-agents, and more. Slash commands are similar but simpler -- shortcuts so you don't have to repeat the same instructions every session. And guidance files like claude.md act as persistent system prompts that shape every interaction in a project.

Sub-agents deserve special attention here. These are specialized mini-versions of the agent that spin up in their own context window to handle a specific task. I'll do things like launch three different code reviewers, each with a different personality and focus area, and see what they find. One checks architecture, another checks implementation details, another looks for edge cases. Hooks round things out by triggering actions based on events -- like automatically running tests after the agent edits a file. The whole point is that the harness is yours to shape. The more you invest in [customizing it](/resources#slide-22), the more leverage you get from every session.`,

  "slide-23": `Context engineering might be the most important meta-skill in this entire presentation. Context is everything you hand over alongside your instruction -- the files, the conversation history, the project knowledge, all of it. And here's the problem: too much context and the model starts forgetting things. Quality degrades. It's like trying to have a focused conversation in a room full of people all talking at once.

Every new session starts with zero context. It's like the movie Memento -- the agent has complete amnesia. That sounds like a limitation, but it's actually a feature once you learn to work with it. The key insight is that sub-agents are one of the best tools for managing context. If you've used 40% of your context window and need a focused task done, you spin up a [sub-agent](/resources#slide-24) that gets its own fresh context window. It goes off, does the work, and comes back with just the answer -- not the entire trail of its thinking. That's enormously efficient.

The practical solutions are straightforward: write important information to files so it persists across sessions, use databases for structured data, and don't be afraid to start fresh. Knowing when to reset your session is genuinely one of the most valuable skills you can develop. If the model seems confused or its responses are declining in quality, that's your signal. Start a new session, give it clean context, and watch the quality jump back up. Getting better at context engineering is kind of the whole game right now.`,

  "slide-24": `This is where you stop being someone who chats with AI and start being someone who orchestrates it. Parallel agents and sub-agent orchestration means you're running multiple AI sessions simultaneously, each working on a different piece of the puzzle. You plan the work, define the outputs, feed each agent the context it needs, and then step back. You come back in twenty minutes and it's done.

GitHub Worktrees are the technical foundation that makes this possible. A worktree lets you have multiple working directories for the same repository, each checked out to a different branch. That means each agent can be editing code without stepping on the others' toes. Tools like Conductor.build help manage this workflow, and memory systems like Beads (from Steve Yegge) help agents maintain awareness across sessions.

The mental shift here is crucial: sub-agents return answers, not their full process. You don't need to see every line of reasoning or every file they touched. You care about the result. This is what it means to coordinate rather than execute. You're the team lead now -- the one who breaks down the problem, assigns the pieces, reviews the outputs, and integrates the work. It's exactly the [digital employee](/resources#slide-04) mental model taken to its logical conclusion, and it's remarkably productive once you get the hang of it.`,

  "slide-25": `This is the tip of the spear -- the bleeding edge of what's possible with AI coding agents right now. We're talking about large numbers of agents running simultaneously, not on your laptop, but on virtual private servers in the cloud. You're no longer sitting in front of Claude Code on your machine. You're renting a server, spinning up five or ten agents in parallel, shoveling in all the instructions you can, and saying "Cool, I'm going out. Come back tomorrow morning."

The economics are wild. Someone open-sourced a setup wizard that makes the pitch: your developer costs five thousand dollars a month, but for under seven hundred dollars you can get ten-plus AI agents working around the clock. Even at $100-200 a month for Claude's Max plan plus $50 for a server, the math is compelling if the output quality is there. Tools like Ralph Wiggum let you spin up agents in serial overnight -- queue up a bunch of tasks before bed and wake up to completed work.

I'm not going to pretend I've fully cracked this level. This is where infrastructure meets intelligence, and the tooling is evolving weekly. But the trajectory is unmistakable. The people who figure out how to reliably orchestrate agent swarms -- how to break down problems, manage context at scale, and quality-check the outputs -- are going to have an extraordinary advantage. It builds directly on everything from [parallel agents](/resources#slide-24) and [context engineering](/resources#slide-23), just cranked up to industrial scale.`,

  "slide-26": `The whole idea behind this presentation is simple: you have access to the greatest learning tool ever created. It won't make fun of you. It won't judge your questions. It has infinite patience. And if you have the curiosity and willingness to engage with it, you can start from anywhere and learn anything.

I'll be honest -- I'm not making money from all these projects yet. I'm still learning. But I'm on the edge of that changing. I'm already past the point where I know I could sell some of what I've built. This presentation tool, for instance -- the [interactive metro map](/resources#project-levelupai) you've been looking at -- is something that didn't exist six months ago, and I built it with AI assistance as someone who isn't a professional developer. That's the kind of thing that becomes possible when you invest the time.

The field is changing so fast that nobody is really that far ahead. That's actually great news. It means the gap between "just getting started" and "pretty capable" is smaller than you think. The only thing that stops you is resistance. If you can open yourself up and say "cool, I can start learning this today," then you're already on the path. My hope is that after seeing this talk, you're a little more curious to do something in your life with AI that goes beyond search. If that's you, mission accomplished.`,

  "project-craigdossantos": `I hadn't hand-coded my own website since maybe the early 2000s. That's two decades of relying on whatever platform was convenient -- WordPress, Squarespace, whatever. But once I started working with AI coding tools, I realized I could actually build something from scratch again. Not just a template with my name slapped on it, but a real personal site that reflects how I think and work.

One of the things I'm proudest of is the automation I built to recover my old writing. I had blog posts scattered across the internet going back twenty years -- some of them on sites I didn't even have access to anymore. I wrote an automation using Claude Code that went to the Wayback Machine, scraped my old posts, cleaned up all the messy HTML, converted everything into clean Markdown, and pulled it all onto my new site. That was genuinely an impossible task before AI. Not technically impossible, but the amount of tedious work involved meant I was never going to do it. With AI, I just described what I wanted and iterated until the script worked. That's a perfect example of [using AI for automation](/resources#slide-14) -- taking something that would have taken weeks of manual effort and turning it into an afternoon project.

This was also my first real experience using [Claude Code on the command line](/resources#slide-16), and it taught me that the barrier to building software isn't knowing syntax. It's having a clear idea of what you want and being willing to iterate.`,

  "project-secretgame": `This project pushed me way beyond what I could do as a developer on my own. The idea is simple: you create a question, answer it yourself, then share a link with your friends. Everyone answers anonymously, but you can only see other people's answers after you've submitted your own. Think of it like a party game version of sharing secrets -- the vulnerability is what makes it fun.

What made this technically ambitious was everything happening under the hood. I needed a persistent back end, user authentication, the concept of separate rooms with their own state, real-time updates -- this was a full-stack web application. Before AI, I couldn't have built this. I could maybe sketch the front end, but wiring up a database, handling auth, managing state across multiple users in different sessions? That was beyond my ability level. With AI assistance, I could describe what I wanted the app to do and work through the implementation piece by piece.

The Secret Game was one of those projects that taught me what's actually possible when you [shift your mental model](/resources#slide-04) from "I need to know how to code this" to "I need to clearly describe what I want and guide the AI through building it." The result is a working app that people actually use -- I drop the link into WhatsApp groups and friends play it. That's the difference between a toy project and something real.`,

  "project-freestyleflow": `This is a mobile app for practicing freestyle rap and improv singing. I've been into freestyle for a while, and I wanted a tool that would let me practice at my own pace -- beats, prompts, timing exercises, the whole thing. Most tools out there are either too rigid or built for people who already know what they're doing. I wanted something that meets you where you are.

Building a mobile app was a different challenge from the web projects. The interaction model is different, the deployment is different, and thinking about how someone uses something on their phone while moving around requires a different kind of design thinking. This project was as much about exploring what AI could do for creative practice as it was about the technical build itself. It connects to a broader theme in this presentation: AI isn't just for productivity and work tasks. It's a tool for building things that serve your creative life too.`,

  "project-ourweunion": `Stef and I needed a wedding website, and I figured -- why not build it myself? I started in Google AI Studio just to see how that worked as a [natural language software tool](/resources#slide-15). I described what I wanted: a clean landing page with our story, event details, and an RSVP form connected to a Google Sheet. Google AI Studio got me a working first draft surprisingly fast.

But I wanted more interactivity, more control over the look and feel, so I took the code into Claude Code and kept iterating. That transition from a natural language builder to a proper development environment is a journey I talk about a lot in this presentation -- it mirrors the progression from [natural language software tools](/resources#slide-15) to [working on the command line](/resources#slide-16). You don't have to make that jump, but when you do, you unlock a lot more flexibility.

The site has been flawless. People are RSVPing directly into our Google Sheet through the Google APIs integration. No subscription service, no monthly fee, no template limitations. Just a custom site that does exactly what we need. For something as personal as a wedding, that feels right.`,

  "project-dialoguedojo": `I wanted a place to practice communication skills -- specifically things like active listening, negotiation techniques, and non-violent communication. The idea is that AI acts as a conversational partner. You give it a scenario, maybe a difficult workplace conversation or a negotiation, and then you practice handling it through voice-to-voice interaction. Afterward, it gives you feedback on what you said, what worked, and what you could improve.

This project sits right at the intersection of [AI as a thought partner](/resources#slide-09) and something more applied. Most people think of AI as a tool for getting answers, but Dialogue Dojo uses it as a practice environment. It's like having a sparring partner who can simulate any kind of conversation and then coach you afterward. The voice-to-voice element is key -- typing out your responses is a completely different experience from actually speaking them, and real communication happens out loud.

Building this taught me a lot about how AI handles multi-turn dialogue, how to structure prompts that create realistic conversational scenarios, and how to get useful feedback out of the model rather than generic praise. It's one of those projects where the AI isn't just helping me build the thing -- it IS the thing.`,

  "project-instantbook": `I read a lot, and I got frustrated with how slow it is to find specific ideas in books I've already read. This app lets you upload an EPUB, and it breaks the book into chapters. You can jump to any section and get a summary, pull out key points, or zoom in for detail. Think of it like having a research assistant who's read every book on your shelf and can instantly brief you on any chapter.

The zoom-in, zoom-out metaphor is what makes this useful to me. Sometimes I want the big picture -- what's this book actually about in two paragraphs? Other times I need to drill into a specific chapter because I remember there was something important there but I can't remember what. Being able to move between those levels of detail changes how I interact with books. It turns reading from a linear experience into something more like navigating a map.

I built this when I was still pretty early in my journey with Claude Code, and honestly, I would do it completely differently now. That's actually one of the most consistent things about [leveling up with AI](/resources#slide-05) -- you build something, learn a ton, and then immediately see how you could do it better. That's not a failure. That's the learning working.`,

  "project-youtubesummary": `This might be the tool I use the most day-to-day. You drop in a YouTube URL -- or even a whole playlist -- and it gives you an overview, key points, takeaways, and hotlinks to the relevant moments. It formats everything like a blog post, something I actually want to read. I can switch into transcript mode if I need the raw text. I can even use Gemini to ask questions about the content.

The real power move, though, is loading multiple videos at once. I can say "here are 45 videos about freestyling," check the ones I'm interested in, and it loads all the transcripts together. Now I can look across multiple videos at the same time, compare what different people are saying, and find patterns. This has been incredibly useful for staying current on topics way faster than surfing YouTube. Even for videos I've already watched, I come back and search through them -- "what was that one thing again?" -- and it's right there.

This project connects directly to the idea of [using AI as a thought partner](/resources#slide-09). It's not just summarizing -- it's giving me a different way to interact with video content. Instead of passively watching, I'm actively navigating and querying. That shift from consumption to interrogation is one of the biggest quality-of-life improvements AI has given me.`,

  "project-videosum": `This is a Mac desktop application for summarizing video content, and it works entirely locally on my machine. I have tons of class recordings and lectures -- some of them hours long -- where I either didn't take notes or want to go back and review specific sections. Video Sum handles all of that.

The back end splits the audio using FFmpeg, then feeds it in chunks to Whisper for transcription. You can't just feed a two-hour audio file to a model all at once -- it has to be chunked up intelligently. Once the transcription is done, I use the Claude API to generate summaries, key points, and structured notes from the content. Because everything runs locally, it's not costing me significant money per use, which matters when you're processing hours and hours of video.

This project taught me a lot about working with audio processing pipelines, API usage, and the practical constraints of [going live with real software](/resources#slide-21). It's one thing to build a demo that works on a single short clip. It's another thing entirely to build something that reliably handles two-hour recordings without falling over. That gap between "it works in the demo" and "it works for real" is where a lot of the actual learning happens.`,

  "project-quota": `This is the project I'm working on the most right now, and it solves a problem that kept bugging me as I built more and more AI-powered apps. Here's the issue: if I wanted to let other people use my [YouTube Summary](/resources#project-youtubesummary) tool or [Instant Book](/resources#project-instantbook), how do I handle billing? Am I going to create a separate subscription for each app? That doesn't make sense for anyone -- not for me as a builder, and definitely not for users who just want to try something.

Quota is a portable wallet system for AI apps. Instead of subscriptions, users put in five bucks and get credits they can spend across any app that's connected to the system. It's like an arcade -- you buy tokens and use them wherever you want. No one has to commit to a $10/month subscription just to try a tool. And as a developer, I don't have to build billing infrastructure into every single app I make. I just plug into Quota and it handles the metering.

This is probably the most technically complex thing I've built, and it represents how far the [project path](/resources#project-craigdossantos) has taken me. From hand-coding a simple portfolio site to building a cross-app billing platform -- that progression happened in months, not years. The AI didn't just help me write the code. It helped me think through the architecture, the user experience, and the business logic in ways I couldn't have done solo.`,

  "project-levelupai": `You're looking at it. This entire presentation is an AI project. Rather than putting together a normal slide deck, I built an interactive website from scratch -- the metro map you've been navigating, the zoom levels, the track-aware keyboard navigation, all of it. Built with React Flow and a whole lot of AI assistance.

Even the images were created through an AI pipeline. I didn't go to an image generator and manually create each slide's visual one at a time. I wrote a script that went through all my talk slides, generated six image options for each one using Nano Banana, built a website to display them, and created a feedback mechanism for selecting the best ones. All I'm doing at that point is making decisions -- the AI handles the generation, the presentation, and the iteration. That workflow mirrors what I talk about in [creating media with AI](/resources#slide-13) and [automation](/resources#slide-14), but applied to my own creative process.

Building this presentation has been a meta-level exercise in everything I'm talking about. The [modes and workflows](/resources#slide-20), the [context engineering](/resources#slide-23), the [skills and customizations](/resources#slide-22) -- I've used all of it to build the thing that explains all of it. If there's one takeaway from the project path, it's this: the best way to learn AI is to build something real with it. Every one of these projects taught me something I couldn't have learned from reading about it. The presentation is both the lesson and the proof.`,
};
